{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-26T16:33:24.656056Z",
     "start_time": "2025-01-26T16:33:22.960784Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T16:33:24.671683Z",
     "start_time": "2025-01-26T16:33:24.656056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DATASET\n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, data, labels, sequence_length, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # X: shape (sequence_length, num_features)\n",
    "        X = self.data[idx : idx + self.sequence_length]\n",
    "        # y: label at the next time step\n",
    "        y = self.labels[idx + self.sequence_length]\n",
    "        return X, y"
   ],
   "id": "10c08a6bf173a7b3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T16:33:24.839017Z",
     "start_time": "2025-01-26T16:33:24.783733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = 'inputs/A'\n",
    "\n",
    "# TODO: Min-max/normalize data\n",
    "# TODO: Generalize to other stocks\n",
    "\n",
    "train_loaders = [] # [dataloader1, dataloader2, ...]\n",
    "test_loaders = []\n",
    "\n",
    "for i in range(1, 16):\n",
    "    data_df = pd.read_csv(f'{path}/{i}.csv', index_col=0)\n",
    "    data_df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "    X_df = data_df.drop(columns='label')\n",
    "    y_df = data_df['label']\n",
    "\n",
    "    X = torch.from_numpy(X_df.values).to(torch.float32)\n",
    "    y = torch.from_numpy(y_df.values).to(torch.float32).type(torch.LongTensor)\n",
    "\n",
    "    dataloader = DataLoader(LSTMDataset(X, y, sequence_length=20), batch_size=1, shuffle=True)\n",
    "\n",
    "    train_loaders.append(dataloader)\n",
    "\n",
    "for i in range(16, 21):\n",
    "    data_df = pd.read_csv(f'{path}/{i}.csv', index_col=0)\n",
    "    data_df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "    X_df = data_df.drop(columns='label')\n",
    "    y_df = data_df['label']\n",
    "\n",
    "    X = torch.from_numpy(X_df.values).to(torch.float32)\n",
    "    y = torch.from_numpy(y_df.values).to(torch.float32).type(torch.LongTensor)\n",
    "\n",
    "    dataloader = DataLoader(LSTMDataset(X, y, sequence_length=20), batch_size=1, shuffle=True)\n",
    "\n",
    "    test_loaders.append(dataloader)\n",
    "\n",
    "print('basa')"
   ],
   "id": "cf5a149e1a685dcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basa\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "c033d5fd5fb04e74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T16:33:25.402053Z",
     "start_time": "2025-01-26T16:33:24.889395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from LSTM import LSTM\n",
    "from LSTM  import LSTMTrainer"
   ],
   "id": "f5ad6c7e6fe733af",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T16:33:26.310474Z",
     "start_time": "2025-01-26T16:33:25.417684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODEL\n",
    "torch.set_anomaly_enabled(True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu'\n",
    "\n",
    "model = LSTM(input_size=6, hidden_size=32, num_layers=1, dropout=0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = LSTMTrainer(model, train_loaders, test_loaders, optimizer, criterion, device, num_epochs=10)"
   ],
   "id": "40cbb588ff481df8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loic4\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T16:36:55.864464Z",
     "start_time": "2025-01-26T16:33:26.329862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRAINING\n",
    "trainer.train()"
   ],
   "id": "ad611b749942061a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad76c730021949d9ab6128a039c34a3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 0:   0%|          | 0/15 [00:00<?, ?period/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da0d424726054ccf8d6c2396b73fc1a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?period/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a6089b151dd4aa2b31c6b688a58cc3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loic4\\PycharmProjects\\NBIStockForecast\\LSTM\\Trainer.py:142: UserWarning: Skipping AUROC calculation for period 11. Shape mismatch: y_true has 2 unique classes, but y_scores has 3 columns.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1:   0%|          | 0/15 [00:00<?, ?period/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "477de7997f0744bfb4d6c650832bd1c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# TRAINING\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NBIStockForecast\\LSTM\\Trainer.py:78\u001B[0m, in \u001B[0;36mLSTMTrainer.train\u001B[1;34m(self, verbose)\u001B[0m\n\u001B[0;32m     76\u001B[0m hidden \u001B[38;5;241m=\u001B[39m (hidden[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdetach(), hidden[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mdetach())\n\u001B[0;32m     77\u001B[0m current_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn(output, target)\n\u001B[1;32m---> 78\u001B[0m \u001B[43mcurrent_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     81\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m current_loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO: Dynamic hidden state batch sizing (cuz is source of EROOR)",
   "id": "ba55b0a3e4fe1892"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f0c3e4a2617d3bf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
