{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-26T09:32:48.067967Z",
     "start_time": "2025-01-26T09:32:46.377288Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T09:32:48.083971Z",
     "start_time": "2025-01-26T09:32:48.070973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DATASET\n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, data, labels, sequence_length, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # X: shape (sequence_length, num_features)\n",
    "        X = self.data[idx : idx + self.sequence_length]\n",
    "        # y: label at the next time step\n",
    "        y = self.labels[idx + self.sequence_length]\n",
    "        return X, y"
   ],
   "id": "10c08a6bf173a7b3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T09:32:48.244415Z",
     "start_time": "2025-01-26T09:32:48.197342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = 'inputs/A'\n",
    "\n",
    "# TODO: Min-max/normalize data\n",
    "# TODO: Generalize to other stocks\n",
    "\n",
    "train_loaders = [] # [dataloader1, dataloader2, ...]\n",
    "test_loaders = []\n",
    "\n",
    "for i in range(1, 16):\n",
    "    data_df = pd.read_csv(f'{path}/{i}.csv', index_col=0)\n",
    "    data_df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "    X_df = data_df.drop(columns='label')\n",
    "    y_df = data_df['label']\n",
    "\n",
    "    X = torch.from_numpy(X_df.values).to(torch.float32)\n",
    "    y = torch.from_numpy(y_df.values).to(torch.float32).type(torch.LongTensor)\n",
    "\n",
    "    dataloader = DataLoader(LSTMDataset(X, y, sequence_length=20), batch_size=64, shuffle=True)\n",
    "\n",
    "    train_loaders.append(dataloader)\n",
    "    test_loaders.append(dataloader)"
   ],
   "id": "cf5a149e1a685dcf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T09:32:48.275294Z",
     "start_time": "2025-01-26T09:32:48.261120Z"
    }
   },
   "cell_type": "code",
   "source": "print(y.dtype)",
   "id": "ad62addae8a13ab6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "c033d5fd5fb04e74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T09:32:48.305512Z",
     "start_time": "2025-01-26T09:32:48.291377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from LSTM import LSTM\n",
    "from LSTM  import LSTMTrainer"
   ],
   "id": "f5ad6c7e6fe733af",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T09:32:49.118437Z",
     "start_time": "2025-01-26T09:32:48.322613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODEL\n",
    "torch.set_anomaly_enabled(True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu'\n",
    "\n",
    "model = LSTM(input_size=6, hidden_size=64, num_layers=1, dropout=0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = LSTMTrainer(model, train_loaders, test_loaders, optimizer, criterion, device, num_epochs=10)"
   ],
   "id": "40cbb588ff481df8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loic4\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T09:32:51.113945Z",
     "start_time": "2025-01-26T09:32:50.034975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRAINING\n",
    "trainer.train()"
   ],
   "id": "ad611b749942061a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba39d4169cba4ba48a9586eb9590ccef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 38, 64), got [1, 64, 64]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# TRAINING\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NBIStockForecast\\LSTM\\Trainer.py:60\u001B[0m, in \u001B[0;36mLSTMTrainer.train\u001B[1;34m(self, verbose)\u001B[0m\n\u001B[0;32m     57\u001B[0m data, target \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice), target\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 60\u001B[0m output, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m hidden \u001B[38;5;241m=\u001B[39m (hidden[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdetach(), hidden[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mdetach())\n\u001B[0;32m     62\u001B[0m current_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn(output, target\u001B[38;5;241m.\u001B[39msqueeze())\n",
      "File \u001B[1;32m~\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\NBIStockForecast\\LSTM\\LSTM.py:16\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hidden)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, hidden):\n\u001B[1;32m---> 16\u001B[0m     out, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     out \u001B[38;5;241m=\u001B[39m out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\n\u001B[0;32m     18\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(out)  \u001B[38;5;66;03m# (batch_size, output_size)\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1119\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m   1116\u001B[0m             hx \u001B[38;5;241m=\u001B[39m (hx[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m), hx[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m   1117\u001B[0m         \u001B[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001B[39;00m\n\u001B[0;32m   1118\u001B[0m         \u001B[38;5;66;03m# the user believes he/she is passing in.\u001B[39;00m\n\u001B[1;32m-> 1119\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_forward_args\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1120\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m   1122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1001\u001B[0m, in \u001B[0;36mLSTM.check_forward_args\u001B[1;34m(self, input, hidden, batch_sizes)\u001B[0m\n\u001B[0;32m    994\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_forward_args\u001B[39m(\n\u001B[0;32m    995\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    996\u001B[0m     \u001B[38;5;28minput\u001B[39m: Tensor,\n\u001B[0;32m    997\u001B[0m     hidden: Tuple[Tensor, Tensor],  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[0;32m    998\u001B[0m     batch_sizes: Optional[Tensor],\n\u001B[0;32m    999\u001B[0m ):\n\u001B[0;32m   1000\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_input(\u001B[38;5;28minput\u001B[39m, batch_sizes)\n\u001B[1;32m-> 1001\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_hidden_size\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1002\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1003\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_expected_hidden_size\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1004\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mExpected hidden[0] size \u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m, got \u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1005\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1006\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_hidden_size(\n\u001B[0;32m   1007\u001B[0m         hidden[\u001B[38;5;241m1\u001B[39m],\n\u001B[0;32m   1008\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_expected_cell_size(\u001B[38;5;28minput\u001B[39m, batch_sizes),\n\u001B[0;32m   1009\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected hidden[1] size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1010\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\NBIStockForecast\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:345\u001B[0m, in \u001B[0;36mRNNBase.check_hidden_size\u001B[1;34m(self, hx, expected_hidden_size, msg)\u001B[0m\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_hidden_size\u001B[39m(\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    340\u001B[0m     hx: Tensor,\n\u001B[0;32m    341\u001B[0m     expected_hidden_size: Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m],\n\u001B[0;32m    342\u001B[0m     msg: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected hidden size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    343\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    344\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hx\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m!=\u001B[39m expected_hidden_size:\n\u001B[1;32m--> 345\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg\u001B[38;5;241m.\u001B[39mformat(expected_hidden_size, \u001B[38;5;28mlist\u001B[39m(hx\u001B[38;5;241m.\u001B[39msize())))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected hidden[0] size (1, 38, 64), got [1, 64, 64]"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO: Dynamic hidden state batch sizing (cuz is source of EROOR)",
   "id": "ba55b0a3e4fe1892"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
